import json
import torch
import re
import os
import numpy as np
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from collections import defaultdict
import time
import networkx as nx  # å¼•å…¥å›¾è®ºåº“ç”¨äºé€»è¾‘æ ¡éªŒ

class TISER_Extension:
    def __init__(self, model_path):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜"""
        print(f"åŠ è½½æ¨¡å‹: {self.model_path}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # æ£€æŸ¥GPUå¹¶è®¾ç½®è®¾å¤‡
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
                
                # å°è¯•ä½¿ç”¨8ä½é‡åŒ–
                try:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        trust_remote_code=True,
                        load_in_8bit=True,  # 8ä½é‡åŒ–
                        low_cpu_mem_usage=True
                    )
                    print("âœ… ä½¿ç”¨8ä½é‡åŒ–åŠ è½½æ¨¡å‹")
                except:
                    # å¦‚æœ8ä½é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šåŠ è½½
                    print("âš ï¸ 8ä½é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šåŠ è½½")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        trust_remote_code=True,
                        low_cpu_mem_usage=True
                    )
            else:
                self.device = torch.device("cpu")
                print("âš ï¸ ä½¿ç”¨CPUï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True
                )
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    # ==========================================
    # [Extension Core] System 2 Thinking é€»è¾‘æ ¡éªŒ
    # ==========================================
    def verify_timeline_logic(self, response):
        """
        è§£ææ¨¡å‹è¾“å‡ºçš„æ—¶é—´çº¿ï¼Œæ„å»ºå›¾è°±å¹¶æ£€æµ‹é€»è¾‘é”™è¯¯ã€‚
        è¿”å›: (is_valid, error_message)
        """
        # 1. æå– <timeline> éƒ¨åˆ†
        timeline_match = re.search(r'<timeline>(.*?)</timeline>', response, re.DOTALL | re.IGNORECASE)
        if not timeline_match:
            # å¦‚æœè¿æ ‡ç­¾éƒ½æ²¡æœ‰ï¼Œè§†ä¸ºæ ¼å¼é”™è¯¯
            return False, "Missing <timeline> tags."
        
        timeline_text = timeline_match.group(1).strip()
        if not timeline_text:
            return False, "Timeline is empty."

        # 2. è§£æäº‹ä»¶å’Œæ—¶é—´ (ä½¿ç”¨æ­£åˆ™æå– TISER æ ¼å¼)
        # æ ¼å¼ç¤ºä¾‹: (Event A) starts at 1990. (Event B) ends at 2000.
        events = defaultdict(dict)
        # åŒ¹é…æ¨¡å¼: (äº‹ä»¶å†…å®¹) starts/ends at æ•°å­—
        pattern = r'\((.*?)\)\s*(starts|ends)\s*at\s*(\d+)'
        matches = re.findall(pattern, timeline_text)
        
        if not matches:
            # å°è¯•å®½æ¾åŒ¹é…ï¼ˆä¸å¸¦æ‹¬å·çš„æƒ…å†µï¼‰
            pattern_loose = r'(.*?)\s*(starts|ends)\s*at\s*(\d+)'
            matches = re.findall(pattern_loose, timeline_text)
            if not matches:
                return False, "Timeline format unparseable. Expected: '(Event) starts/ends at Year'."

        # 3. æ„å»ºæ•°æ®ç»“æ„
        for event_name, type_, year_str in matches:
            event_name = event_name.strip()
            year = int(year_str)
            if type_ == 'starts':
                events[event_name]['start'] = year
            elif type_ == 'ends':
                events[event_name]['end'] = year

        # 4. å›¾è®ºé€»è¾‘æ ¡éªŒ (Graph-Theoretic Checks)
        G = nx.DiGraph() # åˆ›å»ºæœ‰å‘å›¾
        
        for event, times in events.items():
            start = times.get('start')
            end = times.get('end')
            
            # æ ¡éªŒ A: æ—¶é—´å€’æµ (Start > End)
            if start is not None and end is not None:
                if start > end:
                    return False, f"Logical Error: Event '{event}' ends ({end}) before it starts ({start})."
                
                # åœ¨å›¾ä¸­æ·»åŠ è¾¹: Start_Node -> End_Node (ä»£è¡¨æ—¶é—´æµå‘)
                u = f"{event}_start"
                v = f"{event}_end"
                G.add_edge(u, v, weight=(end-start))

        # æ ¡éªŒ B: ç¯æ£€æµ‹ (è™½ç„¶è¿™é‡Œæ˜¯ç®€å•æ—¶é—´è½´ï¼Œä½†å¦‚æœæ¨¡å‹ç”Ÿæˆäº†å¥‡æ€ªçš„å› æœé“¾ï¼Œè¿™é‡Œå¯ä»¥æ‰©å±•)
        try:
            nx.find_cycle(G)
            return False, "Logical Error: Timeline contains a causal loop (Cycle detected in event graph)."
        except nx.NetworkXNoCycle:
            pass
            
        return True, None

    def _raw_generate(self, input_text, max_new_tokens):
        """åº•å±‚çš„ç”Ÿæˆå‡½æ•°ï¼Œä¾› System 2 å¾ªç¯è°ƒç”¨"""
        inputs = self.tokenizer(input_text, return_tensors="pt", truncation=True, max_length=4096)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                temperature=0.0, # ä¿æŒç¡®å®šæ€§
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        if input_text in full_output:
            generated_part = full_output.replace(input_text, "", 1).strip()
        else:
            generated_part = full_output
            
        return generated_part

    def generate_answer_with_system2(self, prompt, max_new_tokens=512, max_retries=1):
        """
        [System 2 Extension] å¸¦æœ‰"ä»‹å…¥ä¿®æ­£"çš„ç”Ÿæˆè¿‡ç¨‹ã€‚
        """
        # --- ç¬¬ä¸€æ¬¡å°è¯• (System 1) ---
        history = prompt
        response = self._raw_generate(history, max_new_tokens)
        
        # è¿è¡Œé€»è¾‘æ ¡éªŒ
        is_valid, error_msg = self.verify_timeline_logic(response)
        
        retry_count = 0
        while not is_valid and retry_count < max_retries:
            # --- è¿›å…¥ System 2 Intervention Mode ---
            retry_count += 1
            
            # æ„å»ºâ€œæŠ¥é”™+é‡è¯•â€çš„ Prompt
            intervention_prompt = (
                f"\n\n[System Alert]: I detected a logical error in your timeline: {error_msg}. "
                "Please regenerate the response (reasoning, timeline, reflection, and answer) correctly."
            )
            
            # å°† é”™è¯¯å›ç­” + æŠ¥é”™ä¿¡æ¯ æ‹¼æ¥åˆ°å†å²ä¸­
            history = history + response + intervention_prompt
            
            # é‡æ–°ç”Ÿæˆ
            response = self._raw_generate(history, max_new_tokens)
            
            # å†æ¬¡æ ¡éªŒ
            is_valid, error_msg = self.verify_timeline_logic(response)
        
        return response, retry_count
    
    def extract_answer(self, response, prompt=None):
        """ä»æ¨¡å‹å“åº”ä¸­æå–ç­”æ¡ˆ"""
        # æ–¹æ³•1: æŸ¥æ‰¾<answer>æ ‡ç­¾
        answer_match = re.search(r'<answer>\s*(.*?)\s*</answer>', response, re.DOTALL | re.IGNORECASE)
        if answer_match:
            return answer_match.group(1).strip()
        
        # æ–¹æ³•2: å¦‚æœæœ‰promptï¼Œå°è¯•æå–promptä¹‹åçš„å†…å®¹ (ç”¨äºfallback)
        if prompt and prompt in response:
            generated = response.split(prompt)[-1].strip()
            lines = [line.strip() for line in generated.split('\n') if line.strip()]
            if lines:
                return lines[-1]
        
        return response.strip()
    
    def calculate_em_f1(self, predicted, ground_truth):
        """è®¡ç®—Exact Matchå’ŒF1åˆ†æ•°"""
        pred = predicted.strip().lower()
        truth = ground_truth.strip().lower()
        
        em = 1 if pred == truth else 0
        
        pred_tokens = set(pred.split())
        truth_tokens = set(truth.split())
        
        if not pred_tokens or not truth_tokens:
            return em, 0.0
        
        common = pred_tokens.intersection(truth_tokens)
        if len(common) == 0:
            return em, 0.0
        
        precision = len(common) / len(pred_tokens)
        recall = len(common) / len(truth_tokens)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return em, f1
    
    def evaluate_dataset(self, dataset_samples, dataset_name, max_samples=None, verbose=False):
        """è¯„ä¼°å•ä¸ªæ•°æ®é›† (é›†æˆ System 2)"""
        if max_samples and len(dataset_samples) > max_samples:
            samples = dataset_samples[:max_samples]
            print(f"  âš ï¸  é™åˆ¶è¯„ä¼°: å‰{max_samples}ä¸ªæ ·æœ¬ (å…±{len(dataset_samples)}ä¸ª)")
        else:
            samples = dataset_samples
        
        total_em, total_f1 = 0.0, 0.0
        processed = 0
        system2_triggered_count = 0 # ç»Ÿè®¡è§¦å‘æ¬¡æ•°
        
        pbar = tqdm(samples, desc=f"  {dataset_name.ljust(20)}", unit="smpl")
        
        for i, sample in enumerate(pbar):
            prompt = sample.get('prompt', '')
            ground_truth = sample.get('answer', '')
            
            if not prompt or not ground_truth:
                continue
            
            try:
                # === ä½¿ç”¨æ–°çš„ System 2 ç”Ÿæˆå‡½æ•° ===
                response, retries = self.generate_answer_with_system2(prompt, max_retries=1)
                
                if retries > 0:
                    system2_triggered_count += 1
                
                # æå–ç­”æ¡ˆ
                predicted = self.extract_answer(response, prompt)
                
                # è®¡ç®—æŒ‡æ ‡
                em, f1 = self.calculate_em_f1(predicted, ground_truth)
                total_em += em
                total_f1 += f1
                processed += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'EM': f'{total_em/processed:.3f}',
                    'Sys2': f'{system2_triggered_count}' # å®æ—¶æ˜¾ç¤ºä»‹å…¥æ¬¡æ•°
                })
                
                # æ¸…ç†æ˜¾å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                if verbose and i < 3:
                    print(f"\n   æ ·æœ¬ {i+1} å¤±è´¥: {e}")
                continue
        
        pbar.close()
        
        if processed == 0:
            return 0.0, 0.0, 0
        
        avg_em = total_em / processed
        avg_f1 = total_f1 / processed
        
        if system2_triggered_count > 0:
            print(f"  [System 2 Stats] è§¦å‘é€»è¾‘ä¿®æ­£: {system2_triggered_count}/{processed} æ¬¡ ({system2_triggered_count/processed:.1%})")
        
        return avg_em, avg_f1, processed

def load_test_data(json_path):
    """åŠ è½½æµ‹è¯•æ•°æ®å¹¶ç»„ç»‡"""
    print(f"åŠ è½½æµ‹è¯•æ•°æ®: {json_path}")
    
    # æ•°æ®é›†åç§°æ˜ å°„
    name_mapping = {
        'tgqa_test': 'TGQA',
        'tempreason_l2_test': 'TempReason (L2)',
        'tempreason_l3_test': 'TempReason (L3)',
        'timeqa_easy_test': 'TimeQA (easy)',
        'timeqa_hard_test': 'TimeQA (hard)',
        'tot_semantic_test': 'ToT_Semantic'
    }
    
    organized_data = defaultdict(list)
    total_samples = 0
    
    with open(json_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc="è¯»å–JSONLæ–‡ä»¶"):
            line = line.strip()
            if line:
                try:
                    sample = json.loads(line)
                    ds_name = sample.get('dataset_name', '').strip().lower()
                    
                    # æ˜ å°„åˆ°æ ‡å‡†åç§°
                    standard_name = name_mapping.get(ds_name, ds_name)
                    organized_data[standard_name].append(sample)
                    total_samples += 1
                except:
                    continue
    
    print(f"âœ… åŠ è½½å®Œæˆ: {total_samples} ä¸ªæ€»æ ·æœ¬")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    table1_datasets = ['TGQA', 'TempReason (L2)', 'TempReason (L3)', 'TimeQA (easy)', 'TimeQA (hard)']
    
    print("\nğŸ“Š æ•°æ®é›†åˆ†å¸ƒ:")
    for ds in table1_datasets:
        cnt = len(organized_data.get(ds, []))
        print(f"  {ds.ljust(25)}: {cnt:>6} ä¸ªæ ·æœ¬")
    
    return organized_data

def print_table1_format(results):
    """ä»¥Table 1æ ¼å¼æ‰“å°ç»“æœ"""
    print("\n" + "="*90)
    print("è¯„ä¼°ç»“æœ (Table 1 æ ¼å¼)")
    print("="*90)
    
    # è¡¨å¤´
    header = f"{'æ•°æ®é›†':<25} {'Exact Match (EM)':<20} {'F1 Score':<20} {'è¯„ä¼°æ ·æœ¬æ•°':<15}"
    print(header)
    print("-" * 85)
    
    table1_order = ['TGQA', 'TempReason (L2)', 'TempReason (L3)', 'TimeQA (easy)', 'TimeQA (hard)']
    
    em_scores, f1_scores = [], []
    
    for ds_name in table1_order:
        if ds_name in results:
            res = results[ds_name]
            em = res['EM']
            f1 = res['F1']
            samples = res['samples_processed']
            total = res['total_samples']
            
            # æ ¼å¼åŒ–è¾“å‡º
            em_str = f"{em:.3f}"
            f1_str = f"{f1:.3f}"
            sample_str = f"{samples}/{total}"
            
            print(f"{ds_name:<25} {em_str:<20} {f1_str:<20} {sample_str:<15}")
            
            if samples > 0:
                em_scores.append(em)
                f1_scores.append(f1)
        else:
            print(f"{ds_name:<25} {'-':<20} {'-':<20} {'0/0':<15}")
    
    # è®¡ç®—å®å¹³å‡
    if em_scores:
        macro_em = np.mean(em_scores)
        macro_f1 = np.mean(f1_scores)
        print("-" * 85)
        print(f"{'Macro Average':<25} {macro_em:.3f:<20} {macro_f1:.3f:<20}")
    
    return macro_em, macro_f1 if em_scores else (0, 0)

def main():
    # ========== é…ç½® ==========
    MODEL_PATH = "C:/Users/Ronnie/Desktop/Python_Test/pythonProject/1"
    TEST_DATA_PATH = "./TISER/data/TISER_test.json"
    
    # è¯„ä¼°è®¾ç½®
    # ä¸ºäº†æµ‹è¯•Extensionæ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œå»ºè®®å…ˆè·‘ä¸€å°éƒ¨åˆ†
    FULL_EVALUATION = False  
    MAX_SAMPLES_PER_DATASET = 20 if not FULL_EVALUATION else None

    print(f"ğŸš€ å¯åŠ¨ Neuro-Symbolic TISER Evaluator (System 2 Enabled)")
    print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"æµ‹è¯•æ•°æ®: {TEST_DATA_PATH}")
    print(f"æ¨¡å¼: {'å®Œæ•´è¯„ä¼°' if FULL_EVALUATION else f'å¿«é€Ÿæµ‹è¯• (æœ€å¤š{MAX_SAMPLES_PER_DATASET}æ ·æœ¬/æ•°æ®é›†)'}")
    print()
    
    # 1. åŠ è½½æµ‹è¯•æ•°æ®
    start_time = time.time()
    test_data = load_test_data(TEST_DATA_PATH)
    data_load_time = time.time() - start_time
    
    # 2. åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = TISER_Extension(MODEL_PATH)
    
    # 3. è¯„ä¼°æ¯ä¸ªæ•°æ®é›†
    print("\n" + "="*90)
    print("å¼€å§‹è¯„ä¼° (Intervention Mode: ON)")
    print("="*90)
    
    results = {}
    table1_datasets = ['TGQA', 'TempReason (L2)', 'TempReason (L3)', 'TimeQA (easy)', 'TimeQA (hard)']
    
    total_evaluation_time = 0
    
    for ds_name in table1_datasets:
        if ds_name not in test_data or len(test_data[ds_name]) == 0:
            print(f"\nâš ï¸  è·³è¿‡: '{ds_name}' æ²¡æœ‰æ•°æ®")
            results[ds_name] = {'EM': 0, 'F1': 0, 'samples_processed': 0, 'total_samples': 0}
            continue
        
        samples = test_data[ds_name]
        total_samples = len(samples)
        
        print(f"\nè¯„ä¼°æ•°æ®é›†: {ds_name}")
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        
        # è¯„ä¼°
        eval_start = time.time()
        em_score, f1_score, processed = evaluator.evaluate_dataset(
            samples, ds_name, MAX_SAMPLES_PER_DATASET, verbose=True
        )
        eval_time = time.time() - eval_start
        
        results[ds_name] = {
            'EM': em_score,
            'F1': f1_score,
            'samples_processed': processed,
            'total_samples': total_samples,
            'eval_time': eval_time
        }
        
        print(f"  å®Œæˆ: EM={em_score:.4f}, F1={f1_score:.4f}, æ—¶é—´={eval_time:.1f}ç§’")
        total_evaluation_time += eval_time
    
    # 4. æ‰“å°Table 1æ ¼å¼çš„ç»“æœ
    print("\n" + "="*90)
    macro_em, macro_f1 = print_table1_format(results)
    
    # 5. ä¿å­˜è¯¦ç»†ç»“æœ (ä½ è¦æ±‚çš„å®Œæ•´é€»è¾‘)
    output_file = "table1_results_detailed_sys2.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'results': results,
            'macro_average': {'EM': macro_em, 'F1': macro_f1},
            'evaluation_settings': {
                'model_path': MODEL_PATH,
                'test_data_path': TEST_DATA_PATH,
                'full_evaluation': FULL_EVALUATION,
                'max_samples_per_dataset': MAX_SAMPLES_PER_DATASET,
                'extension': 'System 2 Thinking (Graph Validation)'
            },
            'timing': {
                'data_loading': data_load_time,
                'total_evaluation': total_evaluation_time,
                'total': time.time() - start_time
            }
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # 6. ä¸è®ºæ–‡ç»“æœå¯¹æ¯”å‚è€ƒ
    print("\n" + "="*90)
    print("è®ºæ–‡ Baseline å‚è€ƒ (TISER Mistral-7B):")
    print("  TGQA: 0.805 | TimeQA(Easy): 0.975 | Macro: 0.887")
    print("  å¦‚æœä½ çš„ EM ç•¥é«˜äºæ­¤ï¼Œè¯´æ˜ Extension æœ‰æ•ˆï¼")

if __name__ == "__main__":
    # è®¾ç½®è­¦å‘Šè¿‡æ»¤
    import warnings
    warnings.filterwarnings("ignore")
    
    # è¿è¡Œä¸»å‡½æ•°
    main()